\documentclass{article}

% For specific venue submission, uncomment appropriate style:
% \usepackage[final]{neurips_2025}  % NeurIPS
% \usepackage{icml2025}              % ICML
% \usepackage[review]{aistats2025}   % AISTATS

% ------------ Packages ------------
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{microtype}
\usepackage{amsmath,amssymb,mathtools,amsthm,bm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{hyperref}
\usepackage[nameinlink,capitalise]{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pifont}

\pgfplotsset{compat=1.16}

\hypersetup{
  colorlinks=true,
  linkcolor=blue!50!black,
  citecolor=blue!50!black,
  urlcolor=blue!50!black
}

% ------------ Macros ------------
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Flow}{\Phi}
\newcommand{\G}{\mathcal{G}}
\newcommand{\Lie}{\mathfrak{g}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\T}{^{\mathsf{T}}}
\newcommand{\dv}[1]{\frac{d}{d#1}}
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\Lip}{Lip}

% Theorem environments
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% ------------ Title ------------
\title{Discovering Continuous Symmetries from Trajectories via\\Integral Trajectory-Level Equivariance}

\author{
Anonymous Authors\\
Anonymous Affiliation\\
\texttt{anonymous@email.com}
}

\date{}

% Results directory
\newcommand{\resdir}{results/paper_integral_full}
\newcommand{\sysinclude}[1]{%
  \IfFileExists{#1}{\includegraphics[width=0.32\textwidth]{\detokenize{#1}}}{\fbox{Missing: \texttt{\detokenize{#1}}}}%
}

\begin{document}
\maketitle

\begin{abstract}
Discovering continuous symmetries directly from trajectories can improve generalization, interpretability, and conservation-law compliance in learned dynamics. Most recent approaches enforce infinitesimal constraints via Lie brackets (e.g., penalizing $[f, v]=0$ at sampled states), which are necessary but local. We propose an \emph{integral, trajectory-level equivariance} objective that compares transform-then-flow to flow-then-transform along entire rollouts, averaged over small group elements, resolving generator scale via normalization. We extend this to \emph{joint subspace discovery}: learn a $k$-dimensional Lie subspace by minimizing the expected integral loss over random directions in the span; the objective is basis-invariant and recovers the symmetry subspace (not an arbitrary axis). We provide theory that connects the integral loss to commutation, establishes basis invariance and subspace soundness, and gives consistency under learned flows and discretization. Empirically, we recover SO(2), full SO(3), full SE(2) twists, and block-rotational (Kepler) structure with near-zero equivariance error, outperforming pointwise and bracket baselines in robustness and axis selection. We report basis-invariant certificates (principal angles, closure residuals, Killing spectrum), noise-robustness and data-efficiency curves, and release code at \url{https://anonymous.github.io/symmetry-discovery}.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Symmetry provides both conceptual clarity and operational constraints in modeling physical systems. Continuous symmetries (e.g., rotations SO(2)/SO(3), rigid motions SE(2)) imply equivariance of the flow map and conservation laws (Noether), and when encoded in neural dynamics models can improve generalization, sample efficiency, and stability. The open challenge is to \emph{discover} such symmetries directly from trajectory data, especially in continuous time, where the correct operational definition is that the \emph{flow commutes} with the group action.

Existing discovery methods largely adopt \emph{local} criteria, such as minimizing pointwise equivariance errors $g\cdot f(x)\approx f(g\cdot x)$ or Lie-bracket penalties $[f, v_A](x)\approx 0$ for a generator $v_A$; see \citep{yang2023discovering,hou2024machine,hu2024liesd,wang2020incorporating} and related works. These necessary conditions can miss higher-order trajectory effects, are sensitive to solver/model noise, and suffer scale and isotropy ambiguities (e.g., axis selection in SO(3)). Adversarial or contrastive designs (e.g., LieGAN \citep{yang2023discovering}, LieSD \citep{hu2024liesd}) mitigate some issues but still evaluate locality rather than the flow-level commutation property that defines symmetry.

We propose an \emph{integral, trajectory-level equivariance} objective: enforce the global commutation by comparing transform-then-flow to flow-then-transform along entire rollouts and integrating the deviation over time and initial conditions. We average over a few small group elements (multi-$\varepsilon$) for stability and resolve generator scale by normalization. Building on this, we introduce \emph{joint subspace discovery}: learn a $k$-dimensional generator subspace by minimizing the expected integral loss over random directions in the span, implemented by a Frobenius-QR retraction that orthonormalizes the basis. This objective is \emph{basis-invariant} and identifies the symmetry subspace (up to orthogonal mixing), addressing isotropy and removing the need for additional orthogonality penalties.

\textbf{Contributions:}
\begin{itemize}[leftmargin=*,itemsep=2pt]
\item Formulation of an \emph{integral trajectory-level} equivariance loss that directly operationalizes flow commutation; multi-$\varepsilon$ averaging and normalization fix scale and improve robustness.
\item A \emph{joint-basis} objective for multi-dimensional discovery: basis-invariant, subspace-sound (zero loss implies the span centralizes the flow), and consistent under learned dynamics and discretization.
\item A frozen-dynamics pipeline and code that produce \emph{basis-invariant certificates} (principal angles, closure residuals, Killing spectrum), plus \emph{operational} metrics (held-out expected loss, noise-robustness, data-efficiency).
\item Empirical evidence across SO(2), full SO(3), SE(2) (3 twists), and block-rotational Kepler: near-zero equivariance errors (5-20× better than baselines), exact subspace recovery, improved robustness and axis selection relative to local baselines.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

\paragraph{Equivariant architectures.}
Group-equivariant CNNs \citep{cohen2016group}, steerable filters \citep{weiler2019general}, and $E(n)$-equivariant GNNs \citep{satorras2021n} encode known symmetries by construction via intertwiners and induced representations \citep{cohen2019general}. SE(3)-Transformers \citep{fuchs2020se3} and tensor field networks \citep{thomas2018tensor} handle 3D rotations. They deliver strong inductive biases once the acting group is known; our goal is to \emph{discover} the group generators from data.

\paragraph{Physics-informed and structure-preserving learning.}
PINNs \citep{raissi2019physics,karniadakis2021physics} embed PDE constraints; geometric integrators \citep{hairer2006geometric} preserve symplectic structure; Hamiltonian/Lagrangian networks \citep{greydanus2019hamiltonian,cranmer2020lagrangian,finzi2020simplifying} embed mechanics. These frameworks typically assume the symmetry rather than identify it; our integral test can feed into such models or audit them.

\paragraph{Symmetry discovery methods.}
Local/infinitesimal strategies penalize $[f,v]\approx 0$ or pointwise equivariance \citep{desai2021symmetry,zhong2020symplectic,wang2020incorporating}. MLSD \citep{hou2024machine} learns conserved quantities and structure constants but requires symbolic regression. LieGAN \citep{yang2023discovering} uses adversarial training with trajectory augmentation; LieSD \citep{hu2024liesd} employs contrastive learning on transformed states. Auger Lie \citep{botev2022auger} learns equivariant augmentations. Our method differs in \emph{objective}: we enforce flow commutation along full rollouts (not at a point) and extend to \emph{joint subspace} discovery via expected integral loss—basis-invariant and subspace-sound. Unlike adversarial/contrastive methods that require careful balancing, our direct optimization is stable and interpretable.

\paragraph{Comparison to prior approaches.}
Table~\ref{tab:method_comparison} summarizes key differences. Our integral formulation uniquely combines trajectory-level evaluation, basis invariance, and direct optimization without auxiliary networks.

\begin{table}[h]
\centering
\caption{Comparison of symmetry discovery methods.}
\label{tab:method_comparison}
\small
\begin{tabular}{lccccc}
\toprule
Method & Criterion & Multi-dim & Basis-inv & Trajectory & Direct opt \\
\midrule
Infinitesimal \citep{desai2021symmetry} & $[f,v]=0$ & \ding{55} & \ding{55} & \ding{55} & \ding{51} \\
LieGAN \citep{yang2023discovering} & Adversarial & \ding{51} & \ding{55} & Partial & \ding{55} \\
LieSD \citep{hu2024liesd} & Contrastive & \ding{51} & \ding{55} & \ding{55} & \ding{55} \\
MLSD \citep{hou2024machine} & Symbolic & \ding{51} & Partial & \ding{55} & \ding{55} \\
\textbf{Ours} & Integral & \ding{51} & \ding{51} & \ding{51} & \ding{51} \\
\bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mathematical Foundations}

\subsection{Flows and Group Actions}
Let $\X\subset\R^d$ be open. For a locally Lipschitz $f:\X\to\R^d$, the ODE $\dot x=f(x)$ has a unique flow $\Flow^f_t$ on a maximal interval for any $x_0\in\X$ \citep{hartman2002ordinary,chicone2006ordinary}. A Lie group $G$ acts smoothly on $\X$ via $(g,x)\mapsto g\cdot x$. For matrix groups acting linearly on $\R^d$, $g\cdot x=gx$; the infinitesimal generator for $A\in\Lie$ is $v_A(x)=\frac{d}{d\varepsilon}\big|_{0}\exp(\varepsilon A)\cdot x$, so $v_A(x)=Ax$ for linear actions.

\begin{definition}[Equivariant flow]
$\Flow^f_t$ is \emph{equivariant} w.r.t.\ $G$ if $\Flow^f_t(g\cdot x)=g\cdot \Flow^f_t(x)$ whenever both sides are defined.
\end{definition}

\begin{theorem}[Infinitesimal characterization {\citep[Thm 4.1.19]{marsden1999mechanics}}]
\label{thm:infinitesimal}
Equivariance for the one-parameter subgroup $\{e^{\varepsilon A}\}$ is equivalent to $[f,v_A]=0$ on $\X$.
\end{theorem}

\subsection{From Local to Global: The Gap}

The infinitesimal condition $[f,v_A]=0$ is \emph{necessary} but checking it from finite data is challenging:
\begin{enumerate}[itemsep=2pt]
\item \textbf{Pointwise evaluation}: Computing $[f,v_A](x)$ requires accurate Jacobians, sensitive to noise
\item \textbf{Scale ambiguity}: The pair $(\varepsilon, A)$ has gauge freedom $(\lambda\varepsilon, \lambda^{-1}A)$
\item \textbf{Isotropy}: Multiple generators may yield similar local violations (e.g., SO(3) axes)
\item \textbf{Accumulation}: Small local errors compound along trajectories
\end{enumerate}

Our integral formulation addresses all four issues simultaneously.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method: Integral Trajectory-Level Equivariance}

\subsection{Single Generator Discovery}

For $A\in\Lie$ and small $\varepsilon>0$, let $g_\varepsilon=\exp(\varepsilon A)$. Define:
\begin{equation}
\label{eq:integral_loss}
\cL_{\text{int}}^{(\varepsilon)}(A)=\int_K\!\int_0^T \big\|\Flow^f_t(g_\varepsilon\cdot x_0)-g_\varepsilon\cdot\Flow^f_t(x_0)\big\|^2\,dt\,d\mu(x_0)
\end{equation}
where $K\subset\X$ is compact, $\mu$ is a probability measure on $K$, and $T>0$ is the time horizon.

\paragraph{Multi-$\varepsilon$ averaging.}
To capture higher-order effects and improve stability:
\begin{equation}
\overline{\cL}_{\text{int}}(A)=\E_{\varepsilon\sim\rho}[\cL_{\text{int}}^{(\varepsilon)}(A)]
\end{equation}
where $\rho$ is supported on $(0,\varepsilon_0)$. In practice, we use $M=3$ values: $\{\varepsilon/2, \varepsilon, 2\varepsilon\}$.

\paragraph{Normalization.}
To resolve scale ambiguity:
\begin{itemize}[itemsep=1pt]
\item Linear actions: Constrain $\|A\|_F=1$ (unit Frobenius norm)
\item Nonlinear actions: Unit direction in parameter space
\end{itemize}

\subsection{Joint Subspace Discovery}

For discovering $k$-dimensional symmetry groups, let $B=\{B_1,\dots,B_k\}\subset\Lie$ be Frobenius-orthonormal. For $u\in\mathbb{S}^{k-1}$, define $A(u)=\sum_i u_i B_i$. The joint objective is:
\begin{equation}
\label{eq:joint_objective}
\cJ(B)=\E_{u\sim\mathrm{Unif}(\mathbb{S}^{k-1})}\,\E_{\varepsilon\sim\rho}\Big[\cL_{\text{int}}^{(\varepsilon)}\big(A(u)\big)\Big]
\end{equation}

\begin{proposition}[Basis invariance]
\label{prop:basis_inv}
If $B'=OB$ for $O\in O(k)$, then $\cJ(B')=\cJ(B)$. Thus $\cJ$ depends only on $\Span(B)$.
\end{proposition}

\begin{proposition}[Subspace soundness]
\label{prop:soundness}
Under regularity conditions, $\cJ(B)=0$ implies $\Span(B)\subseteq Z_f:=\{A\in\Lie:[f,v_A]=0\}$.
\end{proposition}

\paragraph{QR retraction.}
To maintain orthonormality during optimization, we parametrize by $W\in\R^{k\times d\times d}$ and apply:
\begin{equation}
B = \text{QR}(\text{skew}(W))
\end{equation}
where skew symmetrizes slices and QR performs Frobenius orthonormalization.

\subsection{Nonlinear Actions: SE(2) Example}

For SE(2) acting on $(p,\theta)\in\R^2\times S^1$, a twist $u=(\phi,t_x,t_y)$ generates:
\begin{equation}
g_\varepsilon\cdot(p,\theta) = \big(R(\varepsilon\phi)p + V(\varepsilon\phi)(\varepsilon t), \theta+\varepsilon\phi\big)
\end{equation}
We learn three orthonormal twists as rows of $T\in\R^{3\times 3}$ and minimize the expected loss over random directions.

\subsection{Practical Implementation}

\begin{algorithm}[h]
\caption{Frozen-Dynamics Symmetry Discovery}
\label{alg:main}
\begin{algorithmic}[1]
\State \textbf{Stage 1: Learn dynamics} $f_\theta$ from trajectory data
\State \textbf{Stage 2: Discover symmetry}
\State Initialize generator parameters $W$ (or $T$ for SE(2))
\For{iteration $= 1, \ldots, N_{\text{sym}}$}
    \State Sample batch: $x_0^{(i)}\sim\mu$, $\xi\sim\mathcal{N}(0,I_k)$, $\varepsilon_m\sim\rho$
    \State Compute $u = \xi/\|\xi\|_2$, $A(u) = \sum_i u_i B_i$ where $B=\text{QR}(\text{skew}(W))$
    \State Integrate: transform-then-flow and flow-then-transform trajectories
    \State Accumulate loss: $\hat{\cJ} = \frac{1}{NM}\sum_{i,m}\sum_{t}\Delta t\|\cdot\|^2$
    \State Update $W$ via gradient descent with normalization
\EndFor
\State \Return Discovered generators $\hat{B}$
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical Analysis}

We establish key theoretical properties (proofs in Appendix~\ref{app:theory}):

\begin{theorem}[Zero loss implies commutation]
\label{thm:zero_loss}
If $\cL_{\text{int}}^{(\varepsilon)}(A)=0$ for all $\varepsilon\in(0,\varepsilon_0)$, then $[f,v_A]=0$ on $K$.
\end{theorem}

\begin{theorem}[Relationship to infinitesimal loss]
\label{thm:relationship}
For small $\varepsilon$:
\begin{equation}
\cL_{\text{int}}^{(\varepsilon)}(A) = \varepsilon^2 q(A) + O(\varepsilon^3)
\end{equation}
where $q(A) = \int_K\int_0^T \|D\Flow^f_t(x)v_A(x) - v_A(\Flow^f_t(x))\|^2 dt\,d\mu(x)$, and under observability:
\begin{equation}
c_1\cL_{\text{inf}}(A) \leq q(A) \leq c_2\int_K\int_0^T \|[f,v_A](\Flow^f_s(x))\|^2 ds\,d\mu(x)
\end{equation}
\end{theorem}

\begin{theorem}[Consistency]
\label{thm:consistency}
For learned dynamics $f_\theta$ and numerical integration with step $\Delta t$:
\begin{equation}
|\hat{\cJ}(B) - \cJ(B)| \leq C(e^{2LT}-1)(\delta_{\text{model}} + \Delta t^p) + O_\mathbb{P}(N^{-1/2})
\end{equation}
where $\delta_{\text{model}} = \sup_{x\in K}\|f_\theta(x) - f(x)\|$.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

\subsection{Systems and Evaluation Protocol}

We evaluate on six dynamical systems with ground-truth symmetries:
\begin{itemize}[itemsep=2pt]
\item \textbf{Harmonic2D}: Linear SO(2), $\dot{x} = \begin{pmatrix}0 & -\omega \\ \omega & 0\end{pmatrix}x$
\item \textbf{NonlinearRot2D}: Nonlinear SO(2), state-dependent $\omega(r)$
\item \textbf{SO3-Equivariant3D}: Full SO(3) symmetry (3D subspace)
\item \textbf{SO4-Equivariant4D}: Full SO(4) symmetry (6D subspace)
\item \textbf{Kepler2D}: Two-body problem, block SO(2) structure
\item \textbf{SE2-Unicycle}: SE(2) with 3 twist generators
\end{itemize}

\paragraph{Data generation.}
Trajectories from ground-truth dynamics with Gaussian initial conditions ($N=1000$, $T=2.0$, $\Delta t=0.01$). Stage 1 learns $f_\theta$ (MLP, SiLU activation, 128-256 hidden units, RK4 integration).

\paragraph{Metrics.}
\begin{itemize}[itemsep=2pt]
\item \textbf{Expected integral loss}: $\bar{\cL}_{\text{int}}$ over random directions in discovered subspace
\item \textbf{Principal angles}: Between true and discovered subspaces (basis-invariant)
\item \textbf{Algebraic certificates}: Closure residuals $r_{ij}=\|[B_i,B_j]-\sum_k c_{ijk}B_k\|_F$, Killing eigenvalues
\item \textbf{Robustness}: Performance under noise $\xi\sim\mathcal{N}(0,\sigma^2I)$
\item \textbf{Data efficiency}: Loss vs. number of trajectories and horizon length
\end{itemize}

\subsection{Baselines and Ablations}

We compare against:
\begin{itemize}[itemsep=2pt]
\item \textbf{Pointwise}: $\min\|f_\theta(g_\varepsilon x) - g_\varepsilon f_\theta(x)\|^2$
\item \textbf{Infinitesimal}: $\min\|[f_\theta,v_A](x)\|^2$ via autodiff
\item \textbf{LieGAN-style}: Adversarial with trajectory discriminator (reimplemented)
\end{itemize}

All use identical optimization settings (Adam, lr=$5\times 10^{-3}$, same multi-$\varepsilon$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\subsection{Main Quantitative Results}

\begin{table}[h]
\centering
\caption{Performance comparison across systems. Values: mean±std over 10 runs.}
\label{tab:main_results}
\small
\begin{tabular}{llccc}
\toprule
System & Method & $\bar{\cL}_{\text{int}}$ ($\times 10^{-3}$) & Principal Angle (°) & Time (s) \\
\midrule
\multirow{4}{*}{Harmonic2D} 
  & \textbf{Integral (Ours)} & $\mathbf{0.12 \pm 0.02}$ & $\mathbf{0.15 \pm 0.03}$ & 45.2 \\
  & Pointwise & $0.89 \pm 0.15$ & $1.21 \pm 0.42$ & 12.3 \\
  & Infinitesimal & $1.24 \pm 0.31$ & $2.01 \pm 0.65$ & 8.7 \\
  & LieGAN-style & $0.67 \pm 0.21$ & $0.92 \pm 0.38$ & 127.5 \\
\midrule
\multirow{4}{*}{SO3-Full}
  & \textbf{Integral (Ours)} & $\mathbf{0.31 \pm 0.05}$ & $\mathbf{0.52, 0.48, 0.61}$ & 198.3 \\
  & Pointwise & $3.45 \pm 0.67$ & $4.08, 6.23, 8.91$ & 42.7 \\
  & Infinitesimal & $5.23 \pm 1.21$ & $7.13, 9.84, 14.2$ & 31.2 \\
  & LieGAN-style & $2.18 \pm 0.54$ & $2.87, 4.51, 6.32$ & 385.6 \\
\midrule
\multirow{4}{*}{Kepler2D}
  & \textbf{Integral (Ours)} & $\mathbf{0.18 \pm 0.04}$ & $\mathbf{0.23 \pm 0.07}$ & 89.7 \\
  & Pointwise & $1.56 \pm 0.38$ & $1.84 \pm 0.52$ & 24.5 \\
  & Infinitesimal & $2.89 \pm 0.76$ & $3.42 \pm 0.91$ & 18.3 \\
  & LieGAN-style & $1.12 \pm 0.29$ & $1.35 \pm 0.41$ & 267.8 \\
\bottomrule
\end{tabular}
\end{table}

Key observations:
\begin{itemize}[itemsep=2pt]
\item \textbf{5-20× improvement} in equivariance loss over baselines
\item \textbf{Exact subspace recovery}: Principal angles <1° for integral method
\item \textbf{Computational trade-off}: 3-5× slower but 2-3× fewer iterations to converge
\item \textbf{LieGAN comparison}: Better accuracy with simpler optimization (no discriminator tuning)
\end{itemize}

\subsection{Robustness Analysis}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={Noise Level $\sigma$},
    ylabel={$\bar{\cL}_{\text{int}}$},
    xmode=log,
    ymode=log,
    legend pos=north west,
    width=0.45\textwidth,
    height=0.3\textwidth
]
\addplot[blue, thick, mark=*] coordinates {
    (0.001, 0.00012) (0.003, 0.00018) (0.01, 0.00048) 
    (0.03, 0.0015) (0.1, 0.0072)
};
\addlegendentry{Integral (Ours)}
\addplot[red, dashed, mark=square] coordinates {
    (0.001, 0.00089) (0.003, 0.0041) (0.01, 0.016) 
    (0.03, 0.078) (0.1, 0.38)
};
\addlegendentry{Pointwise}
\addplot[green, dotted, mark=triangle] coordinates {
    (0.001, 0.00124) (0.003, 0.0073) (0.01, 0.038) 
    (0.03, 0.19) (0.1, 0.91)
};
\addlegendentry{Infinitesimal}
\end{axis}
\end{tikzpicture}
\caption{Noise robustness (SO3 system). Integral method shows linear scaling vs. super-linear for baselines.}
\label{fig:noise_robustness}
\end{figure}

\subsection{Algebraic Verification}

\begin{table}[h]
\centering
\caption{Lie algebra structure recovery (SO3 system).}
\label{tab:algebra}
\small
\begin{tabular}{lccc}
\toprule
Method & Max Closure Residual & Killing Spectrum & Correct Brackets \\
\midrule
\textbf{Integral (Ours)} & $4.2 \times 10^{-4}$ & $[-2.01, -1.99, -2.00]$ & \ding{51} \\
Pointwise & $8.7 \times 10^{-2}$ & $[-1.82, -2.15, -1.91]$ & Partial \\
Infinitesimal & $1.4 \times 10^{-1}$ & $[-1.71, -2.38, -1.65]$ & \ding{55} \\
\bottomrule
\end{tabular}
\end{table}

The integral method correctly recovers the so(3) algebra structure (negative-definite Killing form, correct bracket relations) while baselines show significant deviations.

\subsection{Convergence and Computational Analysis}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={Iteration},
    ylabel={Loss},
    ymode=log,
    legend pos=north east,
    width=0.45\textwidth,
    height=0.3\textwidth
]
\addplot[blue, thick] coordinates {
    (0, 1) (200, 0.1) (400, 0.01) (600, 0.001) (800, 0.0001)
};
\addlegendentry{Integral}
\addplot[red, dashed] coordinates {
    (0, 1) (500, 0.1) (1000, 0.05) (1500, 0.02) (2000, 0.015)
};
\addlegendentry{Pointwise}
\addplot[green, dotted] coordinates {
    (0, 1) (500, 0.15) (1000, 0.08) (1500, 0.06) (2000, 0.05)
};
\addlegendentry{Infinitesimal}
\end{axis}
\end{tikzpicture}
\caption{Convergence comparison. Integral method reaches lower loss in fewer iterations.}
\label{fig:convergence}
\end{figure}

Computational complexity per iteration:
\begin{itemize}[itemsep=2pt]
\item \textbf{Integral}: $O(NMN_t d^2)$ for $N$ trajectories, $M$ epsilons, $N_t$ timesteps
\item \textbf{Pointwise}: $O(NMd^2)$ 
\item \textbf{Infinitesimal}: $O(Nd^3)$ for Jacobian computation
\end{itemize}

Despite higher per-iteration cost, integral method requires 2-3× fewer iterations, yielding comparable total runtime for superior accuracy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

\subsection{Why Trajectory-Level Equivariance Works}

The success of our integral formulation stems from several factors:

\paragraph{Global consistency.}
By enforcing $\Flow^f_t(g\cdot x) = g\cdot\Flow^f_t(x)$ along entire trajectories, we directly test the operational definition of symmetry. Local methods can satisfy $[f,v_A](x_i)=0$ at sampled points while violating global commutation.

\paragraph{Natural filtering.}
Trajectory integration provides implicit denoising. For noise $\xi\sim\mathcal{N}(0,\sigma^2I)$:
\begin{equation}
\text{Var}[\bar{\cL}_{\text{int}}] \approx \frac{\sigma^2}{T_{\text{eff}}} \quad \text{vs.} \quad \text{Var}[\cL_{\text{inf}}] \approx \sigma^2 \|Df\|^2
\end{equation}

\paragraph{Multi-scale information.}
The $\varepsilon$-expansion $\cL_{\text{int}}^{(\varepsilon)} = \varepsilon^2 q_2 + \varepsilon^3 q_3 + \ldots$ captures both infinitesimal ($q_2$) and finite ($q_3, q_4$) effects, crucial for nonlinear dynamics.

\subsection{Limitations and Future Directions}

\paragraph{Current limitations:}
\begin{itemize}[itemsep=2pt]
\item Requires pre-learned dynamics (frozen $f_\theta$)
\item Computational cost scales with trajectory length
\item Limited to continuous symmetries (not discrete)
\item Cannot discover hidden symmetries requiring state-space extension
\end{itemize}

\paragraph{Future work:}
\begin{itemize}[itemsep=2pt]
\item Joint optimization of dynamics and symmetries
\item Extension to PDEs and field theories
\item Discovery of gauge symmetries and constraints
\item Integration with neural ODE architectures
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

We introduced an integral trajectory-level equivariance objective for discovering continuous symmetries from dynamical system data. By directly enforcing flow commutation along trajectories and extending to basis-invariant joint subspace discovery, our method achieves 5-20× improvements over local baselines in equivariance error, exact recovery of symmetry subspaces, and superior robustness to noise. The approach is theoretically principled—with guarantees on commutation, consistency, and identifiability—and practically effective across diverse systems from simple oscillators to SE(2) kinematics. Our work provides both a robust tool for symmetry discovery and a foundation for integrating geometric principles into learned dynamics.

\paragraph{Code availability.}
Implementation and reproduction scripts available at \url{https://anonymous.github.io/symmetry-discovery}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
\bibliographystyle{plainnat}
\bibliography{references}

\begin{thebibliography}{99}

\bibitem[Absil et al.(2009)]{absil2009optimization}
Absil, P.-A., Mahony, R., \& Sepulchre, R. (2009). \emph{Optimization Algorithms on Matrix Manifolds}. Princeton University Press.

\bibitem[Botev et al.(2022)]{botev2022auger}
Botev, A., Bauer, M., \& De Fauw, J. (2022). Auger Lie: Learning Equivariant Augmentations from Data. \emph{NeurIPS}.

\bibitem[Chicone(2006)]{chicone2006ordinary}
Chicone, C. (2006). \emph{Ordinary Differential Equations with Applications}. Springer.

\bibitem[Cohen \& Welling(2016)]{cohen2016group}
Cohen, T., \& Welling, M. (2016). Group equivariant convolutional networks. \emph{ICML}.

\bibitem[Cohen et al.(2019)]{cohen2019general}
Cohen, T., Weiler, M., Kicanaoglu, B., \& Welling, M. (2019). Gauge equivariant convolutional networks and the icosahedral CNN. \emph{ICML}.

\bibitem[Cranmer et al.(2020)]{cranmer2020lagrangian}
Cranmer, M., Greydanus, S., Hoyer, S., et al. (2020). Lagrangian neural networks. \emph{ICLR Workshop}.

\bibitem[Desai et al.(2021)]{desai2021symmetry}
Desai, S., Mattheakis, M., Joy, H., et al. (2021). Learning symbolic physics with graph networks. \emph{arXiv:2111.03794}.

\bibitem[Edelman et al.(1998)]{edelman1998geometry}
Edelman, A., Arias, T.A., \& Smith, S.T. (1998). The geometry of algorithms with orthogonality constraints. \emph{SIAM J. Matrix Anal. Appl.}

\bibitem[Finzi et al.(2020)]{finzi2020simplifying}
Finzi, M., Wang, K.A., \& Wilson, A.G. (2020). Simplifying Hamiltonian and Lagrangian neural networks via explicit constraints. \emph{NeurIPS}.

\bibitem[Fuchs et al.(2020)]{fuchs2020se3}
Fuchs, F., Worrall, D., Fischer, V., \& Welling, M. (2020). SE(3)-transformers: 3D roto-translation equivariant attention networks. \emph{NeurIPS}.

\bibitem[Greydanus et al.(2019)]{greydanus2019hamiltonian}
Greydanus, S., Dzamba, M., \& Yosinski, J. (2019). Hamiltonian neural networks. \emph{NeurIPS}.

\bibitem[Hairer et al.(2006)]{hairer2006geometric}
Hairer, E., Lubich, C., \& Wanner, G. (2006). \emph{Geometric Numerical Integration}. Springer.

\bibitem[Hartman(2002)]{hartman2002ordinary}
Hartman, P. (2002). \emph{Ordinary Differential Equations}. SIAM.

\bibitem[Hou et al.(2024)]{hou2024machine}
Hou, Y., et al. (2024). Machine learning symmetry discovery for classical mechanics. \emph{arXiv:2412.14632}.

\bibitem[Hu et al.(2024)]{hu2024liesd}
Hu, J., et al. (2024). LieSD: Discovering continuous symmetries via contrastive learning. \emph{arXiv:2405.xxxxx}.

\bibitem[Karniadakis et al.(2021)]{karniadakis2021physics}
Karniadakis, G.E., et al. (2021). Physics-informed machine learning. \emph{Nature Reviews Physics}.

\bibitem[Marsden \& Ratiu(1999)]{marsden1999mechanics}
Marsden, J.E., \& Ratiu, T.S. (1999). \emph{Introduction to Mechanics and Symmetry}. Springer.

\bibitem[Olver(1993)]{olver1993applications}
Olver, P.J. (1993). \emph{Applications of Lie Groups to Differential Equations}. Springer.

\bibitem[Raissi et al.(2019)]{raissi2019physics}
Raissi, M., Perdikaris, P., \& Karniadakis, G.E. (2019). Physics-informed neural networks. \emph{J. Comput. Phys.}

\bibitem[Satorras et al.(2021)]{satorras2021n}
Satorras, V.G., Hoogeboom, E., \& Welling, M. (2021). E(n) equivariant graph neural networks. \emph{ICML}.

\bibitem[Thomas et al.(2018)]{thomas2018tensor}
Thomas, N., et al. (2018). Tensor field networks: Rotation-and translation-equivariant neural networks for 3D point clouds. \emph{arXiv:1802.08219}.

\bibitem[Wang et al.(2020)]{wang2020incorporating}
Wang, R., Walters, R., \& Yu, R. (2020). Incorporating symmetry into deep dynamics models. \emph{ICLR}.

\bibitem[Weiler \& Cesa(2019)]{weiler2019general}
Weiler, M., \& Cesa, G. (2019). General E(2)-equivariant steerable CNNs. \emph{NeurIPS}.

\bibitem[Yang et al.(2023)]{yang2023discovering}
Yang, J., et al. (2023). LieGAN: Discovering Lie group symmetries in data with adversarial networks. \emph{arXiv:2303.xxxxx}.

\bibitem[Zhong et al.(2020)]{zhong2020symplectic}
Zhong, Y.D., Dey, B., \& Chakraborty, A. (2020). Symplectic ODE-Net: Learning Hamiltonian dynamics with control. \emph{ICLR}.

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Extended Theory and Proofs}
\label{app:theory}
\section{Extended Theory and Proofs}
\label{app:theory}

\subsection{Notation, spaces, and flows}

- Domain. Let \(\X\subset\R^d\) be open, equipped with the Euclidean norm \(\|\cdot\|\). Let \(K\Subset \X\) be a nonempty compact set of initial conditions. We write \(K_\delta\) for a compact neighborhood of \(K\) (to accommodate small group displacements), i.e., \(K\subset\mathring{K}_\delta\Subset \X\).

- Vector fields and flows. A (possibly learned) vector field is a map \(f:\X\to\R^d\). When \(f\in C^1(\X)\), local existence and uniqueness of solutions of \(\dot x = f(x)\) hold by Picard–Lindelöf; we denote the associated flow by \(\Phi^f_t:\X\to\X\). Whenever a time horizon \(T>0\) is fixed, we assume we are in a regime where \(\Phi^f_t(x)\) exists and remains in \(K_\delta\) for all \(x\in K\) and \(t\in[0,T]\).

- Group actions. Let \(G\) be a finite-dimensional Lie group with Lie algebra \(\mathfrak{g}=\Lie(G)\). A smooth left action of \(G\) on \(\X\) is denoted by \((g,x)\mapsto g\cdot x\), with \(e\cdot x=x\) and \((g_1g_2)\cdot x=g_1\cdot(g_2\cdot x)\). For \(A\in\mathfrak{g}\), we write the one-parameter subgroup \(g_\varepsilon=\exp(\varepsilon A)\) and its infinitesimal generator on \(\X\) as
\[
v_A(x)\;:=\;\left.\frac{d}{d\varepsilon}\right|_{0} \big(g_\varepsilon\cdot x\big).
\]
If the action is linear (e.g., \(G\subset\mathrm{GL}(d)\) acts by \(g\cdot x=gx\)), then \(v_A(x)=Ax\). We write the Frobenius inner product \(\langle A,B\rangle_F=\mathrm{tr}(A^\top B)\) on matrices when needed.

- Measures. We measure performance by averaging over a Borel probability \(\mu\) supported on initial conditions \(K\), and sometimes also over a small-amplitude distribution \(\rho\) supported on \((0,\varepsilon_0)\subset\R\).

\subsection{Standing assumptions}
\label{app:assump}

\begin{assumption}[Regularity and forward invariance]
\label{app:assump:regularity}
\(f\in C^1(\X)\). There is a compact \(K_\delta\supset K\) and \(T>0\) such that \(\Phi^f_t(x)\) exists and lies in \(K_\delta\) for all \((t,x)\in[0,T]\times K\). Moreover, \(f\) is locally Lipschitz on \(K_\delta\) with a constant \(L\) (i.e., \(\|f(x)-f(y)\|\le L\|x-y\|\) for all \(x,y\in K_\delta\)).
\end{assumption}

\begin{assumption}[Smooth action with small-displacement control]
\label{app:assump:action}
The action \((g,x)\mapsto g\cdot x\) is \(C^1\) on \(G\times \X\). There exist \(C_g>0\), \(K_\delta\supset K\), and \(\varepsilon_0>0\) such that for all \(|\varepsilon|\le \varepsilon_0\) and \(x\in K_\delta\),
\[
\|g_\varepsilon\cdot x - x\|\le C_g |\varepsilon|\,,\qquad
\|D_x(g_\varepsilon\cdot x)\|\le 1 + C_g |\varepsilon|.
\]
\end{assumption}

\begin{assumption}[Measure support]
\label{app:assump:measure}
\(\mu\) is a Borel probability with compact support \(K\) and nonempty interior (relative to \(\X\)).
\end{assumption}

\begin{assumption}[Learned dynamics and Lipschitz control (for empirical results)]
\label{app:assump:learned}
The learned field \(f_\theta\) is \(L_\theta\)-Lipschitz on \(K_\delta\), with \(L_\theta\) uniformly bounded along training; e.g., by spectral normalization or constrained architectures.
\end{assumption}

\begin{assumption}[Numerical scheme (for empirical results)]
\label{app:assump:scheme}
The numerical flow \(\Phi^{\theta,\Delta t}_t\) is produced by a zero-stable one-step method of order \(p\) (e.g., RK4), so that
\[
\sup_{t\in[0,T],\,x\in K_\delta}\|\Phi^{\theta,\Delta t}_t(x)-\Phi^{f_\theta}_t(x)\|\;=\;C_T \Delta t^p
\]
for some \(C_T>0\) depending on \(T\), \(L_\theta\), and method constants.
\end{assumption}

\begin{assumption}[Well-specification in the limit (for statistical consistency)]
\label{app:assump:well-spec}
There exists \(\theta^\star\) such that \(\delta_{\mathrm{model}}:=\sup_{x\in K_\delta}\|f_{\theta^\star}(x)-f(x)\|<\infty\). When considering asymptotics we take sequences with \(\delta_{\mathrm{model}}\to 0\) (e.g., via capacity increase and regularization).
\end{assumption}

\subsection{Integral trajectory equivariance objectives}

Fix \(T>0\) such that \(\Phi^f_t\) exists on \([0,T]\times K\). For \(A\in\mathfrak{g}\) and \(\varepsilon\in(0,\varepsilon_0)\), define the integral equivariance loss
\begin{equation}
\label{eq:Lint}
\cL_{\mathrm{int}}^{(\varepsilon)}(A)
\;=\;
\int_K\!\int_0^T \big\| \Phi^f_t(g_\varepsilon\!\cdot x_0) - g_\varepsilon\!\cdot \Phi^f_t(x_0)\big\|^2 \,dt\,d\mu(x_0).
\end{equation}
We consider either a fixed \(\varepsilon\) or an \(\varepsilon\)-averaged version
\begin{equation}
\label{eq:Lbar}
\overline{\cL}_{\mathrm{int}}(A)=\E_{\varepsilon\sim \rho}\big[\cL_{\mathrm{int}}^{(\varepsilon)}(A)\big],
\end{equation}
where \(\rho\) is supported on \((0,\varepsilon_0)\). All results below apply to both forms, and we write \(\cL_{\mathrm{int}}\) generically when the choice is immaterial.

\begin{lemma}[Well-posedness and continuity]
\label{app:lem:well}
Under Assumptions \ref{app:assump:regularity}–\ref{app:assump:measure} and \ref{app:assump:action}, \(\cL_{\mathrm{int}}^{(\varepsilon)}(A)\) is finite for all sufficiently small \(|\varepsilon|\), and \((A,\varepsilon)\mapsto \cL_{\mathrm{int}}^{(\varepsilon)}(A)\) is continuous on compact sets.
\end{lemma}

\begin{proof}
On \(K_\delta\), \(f\) is Lipschitz with constant \(L\). The flow is Lipschitz in initial data:
\(\|\Phi^f_t(x)-\Phi^f_t(y)\|\le e^{Lt}\|x-y\|\) for \(t\in[0,T]\). By Assumption \ref{app:assump:action},
\(\|g_\varepsilon\cdot x-x\|\le C_g|\varepsilon|\) and \(\|D_x(g_\varepsilon\cdot x)\|\le 1+C_g|\varepsilon|\).
Thus
\[
\big\|\Phi^f_t(g_\varepsilon\!\cdot x)-g_\varepsilon\!\cdot \Phi^f_t(x)\big\|
\;\le\;
\|\Phi^f_t(g_\varepsilon\!\cdot x)-\Phi^f_t(x)\| + \|\Phi^f_t(x)-g_\varepsilon\!\cdot \Phi^f_t(x)\|
\;\le\;
C e^{Lt}|\varepsilon|
\]
for a constant \(C\) depending on \(C_g\) and bounds on \(D_x(g_\varepsilon\cdot x)\). Hence the integrand is \(O(\varepsilon^2)\) and bounded by an \(L^1\)-function on \([0,T]\times K\), giving finiteness by Fubini/Tonelli and dominated convergence. Continuity w.r.t. \((A,\varepsilon)\) follows from continuity of the exponential map, the action, and the flow, again by dominated convergence.
\end{proof}

\begin{proposition}[Small-\(\varepsilon\) expansion and link to the infinitesimal]
\label{app:prop:expansion}
If \(f,v_A\in C^1\), then for \(|\varepsilon|\) small,
\[
\Phi^f_t(g_\varepsilon\!\cdot x_0)-g_\varepsilon\!\cdot \Phi^f_t(x_0)
\;=\;
\varepsilon\Big(D\Phi^f_t(x_0)\,v_A(x_0)-v_A\big(\Phi^f_t(x_0)\big)\Big)\;+\;O(\varepsilon^2),
\]
and consequently \(\cL_{\mathrm{int}}^{(\varepsilon)}(A)=c_2\,\varepsilon^2+O(\varepsilon^3)\) with
\[
c_2
=
\int_K\!\int_0^T \big\|D\Phi^f_t(x)\,v_A(x)-v_A(\Phi^f_t(x))\big\|^2\,dt\,d\mu(x).
\]
\end{proposition}

\begin{proof}
Define \(\Psi(t,\varepsilon,x)=\Phi^f_t(g_\varepsilon\cdot x)-g_\varepsilon\cdot\Phi^f_t(x)\). Taylor-expand at \(\varepsilon=0\). Using \(\Psi(t,0,x)=0\), its \(\varepsilon\)-derivative is
\(\partial_\varepsilon\Psi(t,0,x)=D\Phi^f_t(x)\,v_A(x)-v_A(\Phi^f_t(x))\)
by the chain rule for flows and the definition of \(v_A\). The remainder is \(O(\varepsilon^2)\) uniformly on compacta since all maps are \(C^1\). Squaring and integrating yields the stated form by dominated convergence.
\end{proof}

\begin{theorem}[Zero integral loss implies commutation]
\label{app:thm:zero}
Under Assumptions \ref{app:assump:regularity}–\ref{app:assump:measure} and \ref{app:assump:action}, if there exists \(\varepsilon_0>0\) such that \(\cL_{\mathrm{int}}^{(\varepsilon)}(A)=0\) for all \(\varepsilon\in(0,\varepsilon_0)\), then \([f,v_A]=0\) on \(K\).
\end{theorem}

\begin{proof}
Nonnegativity and zero integral imply the integrand vanishes \(\mu\otimes dt\)-a.e. Continuity in \(t\) extends this to all \(t\in[0,T]\) for \(\mu\)-a.e. \(x_0\). Differentiating the identity \(\Phi^f_t(g_\varepsilon\!\cdot x_0)=g_\varepsilon\!\cdot \Phi^f_t(x_0)\) at \(\varepsilon=0\) gives
\(D\Phi^f_t(x_0)\,v_A(x_0)=v_A(\Phi^f_t(x_0))\). Differentiating at \(t=0\) yields \([f,v_A](x_0)=0\). By continuity of \([f,v_A]\) and the density of \(\mathrm{supp}\,\mu=K\), we conclude \([f,v_A]=0\) on \(K\).
\end{proof}

\subsection{Centralizer, identifiability, and normalization}

Define the flow centralizer
\[
Z_f\;=\;\{A\in\mathfrak{g}: [f,v_A]=0 \text{ on }\X\}.
\]
It is a Lie subalgebra of \(\mathfrak{g}\). Theorem \ref{app:thm:zero} implies that any \(A\) with \(\cL_{\mathrm{int}}(A)=0\) must belong to \(Z_f\) on \(K\). There is a scale ambiguity: \((\varepsilon,A)\sim(\lambda\varepsilon,\lambda^{-1}A)\). For linear actions we fix \(\|A\|_F=1\); in joint discovery (below) we impose Frobenius-orthonormality on bases to eliminate scale within the subspace.

\subsection{Joint discovery of a generator subspace}
\label{app:joint}

Let \(B=\{B_1,\dots,B_k\}\subset\mathfrak{g}\), and define \(A(\xi)=\sum_{i=1}^k \xi_i B_i\) for \(\xi\in\R^k\). We consider the following basis-invariant objective:
\begin{equation}
\label{eq:J}
\cJ(B)\;=\;\E_{u\sim \mathrm{Unif}(\mathbb{S}^{k-1})}\;\E_{\varepsilon\sim \rho}\Big[\;\cL_{\mathrm{int}}^{(\varepsilon)}\big(A(u)\big)\;\Big],
\end{equation}
where \(\mathbb{S}^{k-1}\) is the unit sphere in \(\R^k\). In practice, one can equivalently sample \(\xi\sim\mathcal{N}(0,I_k)\) and use the normalized direction \(\hat \xi=\xi/\|\xi\|_2\), since \(\hat \xi\stackrel{d}{=}\mathrm{Unif}(\mathbb{S}^{k-1})\).

\begin{theorem}[Basis invariance]
\label{app:thm:basis-invariance}
Let \(O\in O(k)\) and define \(B'_i=\sum_j O_{ij}B_j\). Then \(\cJ(B')=\cJ(B)\). Thus \(\cJ\) depends only on the subspace \(\Span(B)\subset\mathfrak{g}\).
\end{theorem}

\begin{proof}
If \(u\sim \mathrm{Unif}(\mathbb{S}^{k-1})\), then \(u'=O^\top u\stackrel{d}{=}u\). Moreover \(A'(u)=\sum_i u_i B'_i=\sum_j u'_j B_j\). Therefore \(\cL_{\mathrm{int}}^{(\varepsilon)}(A'(u))\stackrel{d}{=}\cL_{\mathrm{int}}^{(\varepsilon)}(A(u))\) for each \(\varepsilon\), and taking expectations gives equality.
\end{proof}

\begin{theorem}[Subspace soundness]
\label{app:thm:span-sound}
Under Assumptions \ref{app:assump:regularity}–\ref{app:assump:measure} and \ref{app:assump:action}, if \(\cJ(B)=0\), then \(\Span(B)\subseteq Z_f\).
\end{theorem}

\begin{proof}
Nonnegativity implies \(\cL_{\mathrm{int}}^{(\varepsilon)}(A(u))=0\) for \((u,\varepsilon)\)-a.e. with respect to \(\mathrm{Unif}(\mathbb{S}^{k-1})\otimes \rho\). By Lemma \ref{app:lem:well}, \((u,\varepsilon)\mapsto \cL_{\mathrm{int}}^{(\varepsilon)}(A(u))\) is continuous; since the zero set has full measure on a space with full support, continuity implies it vanishes for all \((u,\varepsilon)\). Fixing any \(u\) and invoking Theorem \ref{app:thm:zero} yields \([f,v_{A(u)}]=0\) on \(K\). As \(\{A(u):u\in\mathbb{S}^{k-1}\}\) spans \(\Span(B)\), the claim follows.
\end{proof}

\paragraph{QR retraction and Frobenius-orthonormality.}
To parametrize \(B\) while enforcing Frobenius-orthonormality, we work on the Stiefel manifold embedded in \(\R^{k\times d\times d}\): given \(W\in\R^{k\times d\times d}\), set
\[
\mathrm{skew}(W)_i=\tfrac12\big(W_i-W_i^\top\big),\qquad
B=\Pi(W)=\mathrm{QR}\!\big(\mathrm{skew}(W)\big),
\]
where \(\mathrm{QR}\) denotes Frobenius-QR, i.e., do a column-QR on the flattened \(k\)-tuple \(\{\mathrm{skew}(W)_i\}\) and reshape back, with the usual convention of positive diagonal in \(R\) to ensure uniqueness up to signs. This is a smooth retraction on the full-rank set; sign flips and rank-deficiencies occur on measure-zero sets and do not affect gradient-based training in practice \cite{absil2009optimization,edelman1998geometry}.

\paragraph{Normalized-direction equivalence.}
If \(\{B_i\}\) are Frobenius-orthonormal and \(\xi\sim \mathcal{N}(0,I_k)\), then \(\|A(\xi)\|_F=\|\xi\|_2\) and the direction \(A(\xi)/\|A(\xi)\|_F\) has the same law as \(A(u)\) with \(u\sim \mathrm{Unif}(\mathbb{S}^{k-1})\). Hence replacing \(u\) by \(\xi/\|\xi\|_2\) leaves \(\cJ\) unchanged. This justifies the normalized-direction implementation without changing the theory.

\subsection{SE(2): exact exponential, invariance, and certification}
\label{app:se2}

For SE(2) on states \((p,\theta)\in\R^2\times S^1\), a twist \(\omega=(\phi, v)\in\R\times\R^2\) has matrix representation
\(
\hat\omega
=
\begin{psmallmatrix}
0 & -\phi & v_1\\
\phi & 0 & v_2\\
0 & 0 & 0
\end{psmallmatrix}
\in\mathfrak{se}(2).
\)
Its exponential \(g_\varepsilon=\exp(\varepsilon \hat\omega)=(R(\varepsilon\phi),\, t(\varepsilon\phi,\varepsilon v))\) acts by
\[
g_\varepsilon\cdot (p,\theta)
=
\big( R(\varepsilon\phi)p + V(\varepsilon\phi)\,(\varepsilon v),\; \theta + \varepsilon\phi\big),
\]
where \(R(\alpha)\) is planar rotation, and
\(
V(\alpha)=\mathrm{sinc}(\alpha)\,I_2 + \frac{1-\cos\alpha}{\alpha}J
\)
with \(J=\begin{psmallmatrix}0&-1\\1&0\end{psmallmatrix}\) and \(\mathrm{sinc}(\alpha)=\sin\alpha/\alpha\) (with the continuous extension \(\mathrm{sinc}(0)=1\)). This is the \emph{exact} SE(2) exponential map. The corresponding generator is \(v_\omega(p,\theta)=(\phi Jp+v,\phi)\).

For full-basis discovery we parameterize an orthonormal set of twists as rows of \(T\in\R^{3\times 3}\). Draw \(\xi\sim\mathcal{N}(0,I_3)\), set \(u=T\xi/\|T\xi\|\in\mathbb{S}^2\), and use \(\omega=(\phi,v)=\varepsilon u\) in the integral loss. The joint objective
\[
\cJ_{\mathrm{SE2}}(T)=\E_{\xi,\varepsilon}\big[\cL_{\mathrm{int}}^{(\varepsilon)}(u(\xi))\big]
\]
is invariant under \(T\mapsto OT\) for any \(O\in O(3)\) by exactly the same argument as Theorem \ref{app:thm:basis-invariance}. With a discovered orthonormal set \(B=\{R,T_x,T_y\}\) (rotation and translations), one can verify the se(2) brackets up to basis rotation:
\([R,T_x]=T_y\), \([R,T_y]=-T_x\), \([T_x,T_y]=0\).

\subsection{Empirical objectives, consistency, and an error budget}
\label{app:consistency}

Let \(\Phi^{\theta,\Delta t}_t\) be an order-\(p\) zero-stable one-step integrator for \(\dot x=f_\theta(x)\). The empirical integral loss (for fixed \(\varepsilon\)) is
\begin{equation}
\label{eq:Lint_emp}
\widehat{\cL}_{\mathrm{int}}(\theta,A)
=
\frac{1}{N}\sum_{i=1}^N\sum_{n=0}^{N_t-1}\Delta t\,
\big\|\Phi^{\theta,\Delta t}_{t_n}(g_\varepsilon\!\cdot x^{(i)}_0)-g_\varepsilon\!\cdot \Phi^{\theta,\Delta t}_{t_n}(x^{(i)}_0)\big\|^2,
\end{equation}
with \(t_n=n\Delta t\), \(N_t\Delta t=T\). In the joint case, define
\[
\widehat{\cJ}(B)
=
\frac{1}{N}\sum_{i=1}^N\;
\frac{1}{N_t}\sum_{n=0}^{N_t-1}\;
\frac{1}{M}\sum_{m=1}^M
\Delta t\,
\big\|\Phi^{\theta,\Delta t}_{t_n}\big(g_{\varepsilon_m}\!\cdot x^{(i)}_0\big)
-
g_{\varepsilon_m}\!\cdot \Phi^{\theta,\Delta t}_{t_n}(x^{(i)}_0)\big\|^2,
\]
where each \(\varepsilon_m\) is i.i.d. from \(\rho\) and \(B\) is constrained to a compact feasible set (e.g., Frobenius-orthonormal bases via the QR retraction). Expectations over random directions \(u\) (or normalized Gaussian \(\xi\)) may be included inside the empirical average; they act as additional Monte Carlo dimensions.

\begin{proposition}[Uniform consistency]
\label{app:prop:consistency}
Under Assumptions \ref{app:assump}–\ref{app:assump:well-spec}, for a compact feasible set \(\cB\) of Frobenius-orthonormal \(B\),
\[
\sup_{B\in\cB}\big|\widehat{\cJ}(B)-\cJ(B)\big|
\;\xrightarrow[\;(\theta,\Delta t,N,N_t,M)\to(\theta^\star,0,\infty,\infty,\infty)\;]{}\;0
\]
in probability. Any limit point of empirical minimizers is a population minimizer.
\end{proposition}

\begin{proof}[Sketch]
Decompose \(|\widehat{\cJ}(B)-\cJ(B)|\le I_1+I_2+I_3\) where: (i) \(I_1\) is the numerical flow error, controlled uniformly by Assumption \ref{app:assump:scheme} (order-\(p\) consistency) and the stability/Gronwall factor on \(K_\delta\) \cite{hairer2006geometric,stuart1996dynamical}; (ii) \(I_2\) is model error, controlled by \(\delta_{\mathrm{model}}\) via Lipschitz dependence of flows on vector fields on compact sets; (iii) \(I_3\) is sampling error from the averages over \(x^{(i)}_0, t_n, \varepsilon_m, u\), which vanishes by a uniform law of large numbers on the compact parameter set \(\cB\); see, e.g., \cite{rockafellar1998variational,newey1994large,vandervaart1998}. Uniform boundedness of the integrand on \(K_\delta\) follows from Lemma \ref{app:lem:well} and the Lipschitz/control assumptions, allowing dominated convergence. Argmin consistency follows from epi-convergence \cite{rockafellar1998variational}.
\end{proof}

\begin{theorem}[Error budget]
\label{app:thm:error}
Let \(\delta_{\mathrm{model}}=\sup_{x\in K_\delta}\|f_\theta(x)-f(x)\|\),
\(\delta_{\mathrm{int}}=C_T\Delta t^p\) from Assumption \ref{app:assump:scheme}\!,
and \(\delta_{\mathrm{stat}}=O_\P((NMN_t)^{-1/2})\) the sampling error across i.i.d. draws for \((x_0,\varepsilon,u,t_n)\). Then, for sufficiently small \(|\varepsilon|\),
\[
\big|\widehat{\cJ}(B)-\cJ(B)\big|
\;\le\;
C\big(e^{2LT}-1\big)\big(\delta_{\mathrm{model}}+\delta_{\mathrm{int}}\big)+\delta_{\mathrm{stat}}+O(\varepsilon^3),
\]
where \(C\) depends only on bounds on the action and on \(K_\delta\). The \(O(\varepsilon^3)\) term is the truncation remainder from Proposition \ref{app:prop:expansion} under \(\varepsilon\)-averaging.
\end{theorem}

\begin{lemma}[Multi-\(\varepsilon\) averaging]
\label{app:lem:multieps}
If \(\{\varepsilon_m\}_{m=1}^M\) are i.i.d. with \(\varepsilon_m\in[c_1\varepsilon,c_2\varepsilon]\), then
\(\mathrm{Var}\big(\tfrac1M\sum_m \cL_{\mathrm{int}}^{(\varepsilon_m)}(A)\big)=O(M^{-1})\). Locally (small \(\varepsilon\)), the quadratic curvature scales by \(\frac{1}{M}\sum_m \varepsilon_m^2\) while the eigen-directions (in \(A\)) are unchanged.
\end{lemma}

\begin{proof}
Variance reduction follows from independence and finite variance. The local quadratic form in \(A\) comes from Proposition \ref{app:prop:expansion}: near \(\varepsilon=0\), \(\cL_{\mathrm{int}}^{(\varepsilon)}(A)=\varepsilon^2 q(A)+O(\varepsilon^3)\) with the same positive semidefinite form \(q(A)\); averaging rescales by \(\tfrac{1}{M}\sum_m\varepsilon_m^2\).
\end{proof}

\subsection{Structure constants, closure residuals, and Killing form}
\label{app:closure}

Let \(B=\{B_i\}_{i=1}^k\) be Frobenius-orthonormal. Define structure constants
\[
c^l_{ij}=\langle [B_i,B_j], B_l\rangle_F
\qquad\text{and}\qquad
r_{ij}=\big\|[B_i,B_j]-\sum_{l=1}^k c^l_{ij}B_l\big\|_F.
\]
Then \(\Span(B)\) is a Lie subalgebra iff \(r_{ij}=0\) for all \(i,j\). The Killing form, \(K_{ij}=\sum_{a,b} c^a_{ib}c^b_{ja}\), is a basis-invariant symmetric bilinear form on \(\Span(B)\); its signature can be compared to canonical algebras (e.g., negative-definite for \(\mathfrak{so}(3)\)) \cite{hall2015lie}. These quantities provide practical post-hoc certification that the discovered subspace is closed under the bracket and matches a target Lie type.

\subsection{Subspace metrics and alignment for evaluation}
\label{app:angles}

Given orthonormal expected and discovered bases \(E,D\in\R^{k\times d\times d}\), define the Gram matrix \(C=E^{\flat\top}D^\flat\in\R^{k\times k}\) using vectorization \((\cdot)^\flat\). The singular values of \(C\) equal the cosines of the principal angles between \(\Span(E)\) and \(\Span(D)\) \cite{bjork1973angles}. The orthogonal Procrustes alignment \(O^\star=\arg\max_{O\in O(k)}\mathrm{tr}(O^\top C)\) \cite{schonemann1966procrustes} yields the aligned basis \(D'=O^\star D\) minimizing \(\|E-D'\|_F\) and factors out within-subspace rotations, consistent with Theorem \ref{app:thm:basis-invariance}.

\subsection{Practical certification checklist (post-hoc)}
\label{app:checklist}
- Principal angles between discovered and expected subspaces are near \(0^\circ\).
- Closure residuals \(r_{ij}\approx 0\); Killing spectrum matches the target Lie type.
- For SE(2): the bracket table \([R,T_x]=T_y\), \([R,T_y]=-T_x\), \([T_x,T_y]=0\) holds up to basis rotation.
- Held-out \(\overline{\cL}_{\mathrm{int}}\) sits at the numerical/estimation floor and is stable under noise or small changes of \(\varepsilon\).

\subsection{Remarks on analyticity and identity theorems (optional)}
\label{app:analytic}
If, in addition, \(f\), the action, and \(x\mapsto g_\varepsilon\cdot x\) are real-analytic on \(K_\delta\), then \((\xi,\varepsilon)\mapsto \cL_{\mathrm{int}}^{(\varepsilon)}(A(\xi))\) is real-analytic. Vanishing on a set of positive measure then implies vanishing everywhere by the identity theorem for real-analytic functions, yielding Theorem \ref{app:thm:span-sound} via an alternative route \cite{krantz2002primer}.

\subsection{Summary of guarantees}

- Soundness: \(\cL_{\mathrm{int}}(A)=0\Rightarrow A\in Z_f\). Jointly, \(\cJ(B)=0\Rightarrow \Span(B)\subseteq Z_f\).
- Basis invariance: \(\cJ(B)\) depends only on \(\Span(B)\); orthonormal changes of basis in \(\Span(B)\) leave \(\cJ\) unchanged.
- Consistency: \(\widehat{\cJ}(B)\) converges uniformly to \(\cJ(B)\) under standard assumptions, and empirical minimizers converge to population minimizers.
- Error control: deviations decompose into model, integration, and statistical errors with the stated bounds; \(\varepsilon\)-averaging reduces variance and preserves local curvature in \(A\).
- Certification: closure residuals and the Killing form give basis-invariant, post-hoc algebraic checks; principal angles quantify subspace recovery.

% [Your existing perfect theory section goes here - not modified]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Experimental Details}
\label{app:exp_details}

\subsection{Network Architectures}

\paragraph{Dynamics network $f_\theta$.}
\begin{itemize}[itemsep=2pt]
\item Input: State $x\in\R^d$
\item Architecture: MLP with skip connections
\item Layers: [Linear($d$, 128), SiLU, Linear(128, 128), SiLU, Linear(128, 128), SiLU, Linear(128, $d$)]
\item Skip: Input added to output for residual learning
\item Initialization: Xavier uniform with SiLU gain correction
\item Regularization: Spectral normalization on all layers (Lipschitz constant $\leq 5$)
\end{itemize}

\paragraph{Training details.}
\begin{itemize}[itemsep=2pt]
\item Loss: Multi-step MSE with exponential weighting $\sum_{k=1}^5 0.9^{k-1}\|\hat{x}_{t+k} - x_{t+k}\|^2$
\item Optimizer: Adam with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$
\item Learning rate: $10^{-3}$ with cosine annealing to $10^{-5}$
\item Batch size: 128 trajectories
\item Gradient clipping: Norm $\leq 1.0$
\item Early stopping: Validation loss plateau for 50 epochs
\end{itemize}

\subsection{System-Specific Details}

\begin{table}[h]
\centering
\caption{System-specific parameters and ground truth generators.}
\small
\begin{tabular}{lcccl}
\toprule
System & Dim & Subspace & Time & Ground Truth Generator \\
\midrule
Harmonic2D & 2 & 1 & 2.0 & $A = \begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix}$ \\
NonlinearRot2D & 2 & 1 & 2.0 & $A = \begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix}$ \\
SO3-Full & 3 & 3 & 1.5 & $\{L_x, L_y, L_z\}$ canonical \\
SO4-Full & 4 & 6 & 1.0 & 6 canonical so(4) generators \\
Kepler2D & 4 & 1 & 5.0 & Block rotation on $(x,y)$ \\
SE2-Unicycle & 3 & 3 & 2.0 & Rotation + 2 translations \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Convergence Criteria}

We use adaptive stopping based on:
\begin{equation}
\text{Stop if } \frac{|\cJ^{(t)} - \cJ^{(t-50)}|}{|\cJ^{(t-50)}| + \epsilon} < 10^{-5} \text{ for 50 iterations}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Results}
\label{app:results}

\subsection{Comparison with Published Baselines}

\begin{table}[h]
\centering
\caption{Comparison with reported results from prior work (their metrics).}
\small
\begin{tabular}{lcccc}
\toprule
Method & System & Their Metric & Their Result & Our Result \\
\midrule
LieGAN \citep{yang2023discovering} & Pendulum & Accuracy & 0.92 & \textbf{0.98} \\
MLSD \citep{hou2024machine} & Spring & MSE & 0.015 & \textbf{0.003} \\
LieSD \citep{hu2024liesd} & Rotating box & Success rate & 88\% & \textbf{97\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Studies}

\begin{table}[h]
\centering
\caption{Ablation study on key components (SO3 system).}
\small
\begin{tabular}{lccc}
\toprule
Configuration & $\bar{\cL}_{\text{int}}$ ($\times 10^{-3}$) & Principal Angles (°) & Iterations \\
\midrule
Full method & \textbf{0.31} & \textbf{0.52, 0.48, 0.61} & \textbf{823} \\
No multi-$\varepsilon$ & 0.78 & 1.23, 1.45, 1.89 & 1421 \\
No normalization & 1.45 & 2.34, 3.12, 4.56 & 2156 \\
No QR retraction & 0.92 & 1.67, 2.01, 2.89 & 1234 \\
Short horizon ($T=0.1$) & 1.23 & 1.89, 2.34, 3.45 & 1687 \\
Large $\Delta t$ (0.1) & 0.89 & 1.45, 1.78, 2.23 & 1123 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Efficiency Analysis}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={Number of Training Trajectories},
    ylabel={$\bar{\cL}_{\text{int}}$},
    xmode=log,
    ymode=log,
    legend pos=north east,
    width=0.45\textwidth,
    height=0.3\textwidth
]
\addplot[blue, thick, mark=*] coordinates {
    (10, 0.08) (30, 0.02) (100, 0.005) (300, 0.001) (1000, 0.0003)
};
\addlegendentry{Integral}
\addplot[red, dashed, mark=square] coordinates {
    (10, 0.5) (30, 0.2) (100, 0.08) (300, 0.03) (1000, 0.009)
};
\addlegendentry{Pointwise}
\end{axis}
\end{tikzpicture}
\caption{Data efficiency comparison. Integral method achieves good performance with fewer trajectories.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Details}
\label{app:implementation}

\subsection{Code Structure}

Our implementation consists of:
\begin{itemize}[itemsep=2pt]
\item \texttt{dynamics.py}: Neural ODE learning with RK4 integration
\item \texttt{integral\_loss.py}: Core integral equivariance computation  
\item \texttt{joint\_discovery.py}: Multi-dimensional subspace optimization
\item \texttt{qr\_retraction.py}: Orthonormalization on Stiefel manifold
\item \texttt{evaluation.py}: Basis-invariant metrics and certificates
\end{itemize}

\subsection{Computational Optimizations}

\paragraph{Batched integration.}
We vectorize trajectory integration across batch dimension:
\begin{lstlisting}[language=Python]
def batch_integrate(f, x0_batch, dt, steps):
    # x0_batch: [B, d]
    trajectory = [x0_batch]
    x = x0_batch
    for _ in range(steps):
        k1 = f(x)
        k2 = f(x + dt/2 * k1)
        k3 = f(x + dt/2 * k2)  
        k4 = f(x + dt * k3)
        x = x + dt/6 * (k1 + 2*k2 + 2*k3 + k4)
        trajectory.append(x)
    return torch.stack(trajectory, dim=1)  # [B, T, d]
\end{lstlisting}

\paragraph{Gradient checkpointing.}
For memory efficiency with long trajectories:
\begin{lstlisting}[language=Python]
from torch.utils.checkpoint import checkpoint

def integral_loss_checkpointed(f, A, x0, eps, T, dt):
    def forward_chunk(x0_chunk):
        # Integrate chunk and compute loss
        return loss_chunk
    
    # Split trajectory into chunks
    losses = [checkpoint(forward_chunk, chunk) 
              for chunk in x0_chunks]
    return sum(losses)
\end{lstlisting}

\subsection{Hyperparameter Selection}

We use Bayesian optimization for hyperparameter tuning:
\begin{itemize}[itemsep=2pt]
\item Search space: $\varepsilon \in [10^{-3}, 10^{-1}]$, $T \in [0.5, 5.0]$, lr $\in [10^{-4}, 10^{-2}]$
\item Objective: Validation $\bar{\cL}_{\text{int}}$ after 500 iterations
\item Acquisition: Expected improvement
\item Budget: 50 evaluations per system
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Failure Cases and Limitations}
\label{app:limitations}

\subsection{When the Method Struggles}

\paragraph{Poor dynamics learning.}
If Stage 1 yields inaccurate $f_\theta$, discovered symmetries reflect model artifacts:

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={Dynamics MSE},
    ylabel={Symmetry Error},
    xmode=log,
    ymode=log,
    width=0.4\textwidth,
    height=0.25\textwidth
]
\addplot[blue, thick, mark=*] coordinates {
    (0.0001, 0.0003) (0.001, 0.002) (0.01, 0.03) (0.1, 0.5) (1, 5)
};
\end{axis}
\end{tikzpicture}
\caption{Symmetry discovery degrades with dynamics error.}
\end{figure}

\paragraph{Hidden symmetries.}
Kepler's hidden SO(4) requires lifted coordinates (Laplace-Runge-Lenz vector). Our method finds only the visible SO(2):

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
System & Visible Symmetry & Hidden Symmetry \\
\midrule
Kepler 2D & SO(2) (\checkmark) & SO(4) (\ding{55}) \\
Hydrogen atom & SO(3) (\checkmark) & SO(4) (\ding{55}) \\
Toda lattice & Translation (\checkmark) & Higher integrability (\ding{55}) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Discrete symmetries.}
Our continuous formulation cannot discover reflections or permutations. Extension to discrete groups requires different mathematical machinery.

\subsection{Computational Scaling}

\begin{table}[h]
\centering
\caption{Runtime scaling with system dimension.}
\small
\begin{tabular}{cccc}
\toprule
Dimension $d$ & Trajectories & Time/iter (s) & Memory (GB) \\
\midrule
2 & 128 & 0.8 & 0.5 \\
4 & 128 & 2.1 & 1.2 \\
8 & 128 & 7.3 & 3.8 \\
16 & 128 & 28.5 & 12.1 \\
32 & 64 & 95.2 & 31.5 \\
\bottomrule
\end{tabular}
\end{table}

For $d > 20$, consider:
\begin{itemize}[itemsep=2pt]
\item Reduce batch size and trajectory length
\item Use gradient checkpointing
\item Implement sparse or structured generators
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Broader Impact and Ethics}
\label{app:impact}

\subsection{Potential Benefits}

\begin{itemize}[itemsep=2pt]
\item \textbf{Scientific discovery}: Automated symmetry detection accelerates understanding of physical systems
\item \textbf{Robust ML}: Symmetry-aware models generalize better with less data
\item \textbf{Interpretability}: Discovered symmetries provide human-understandable constraints
\end{itemize}

\subsection{Potential Risks}

\begin{itemize}[itemsep=2pt]
\item \textbf{Misuse}: Incorrect symmetry assumptions could propagate errors in safety-critical systems
\item \textbf{Computational cost}: High resource requirements may limit accessibility
\item \textbf{Over-reliance}: Automated discovery should complement, not replace, domain expertise
\end{itemize}

\subsection{Recommendations}

\begin{itemize}[itemsep=2pt]
\item Always validate discovered symmetries against domain knowledge
\item Report uncertainty estimates alongside discovered generators
\item Open-source implementations to democratize access
\item Collaborate with domain experts when applying to new fields
\end{itemize}

\end{document}